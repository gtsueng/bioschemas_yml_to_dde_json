{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDE-compatible json converter\n",
    "\n",
    "This script converts ingests the yml files and outputs jsonschema that should be mostly DDE-compatible. Note that there are exceptions as some missing logic is not yet in place. The DDE already has a copy of schema.org ingested; hence, it should not be necessary to include enums of all schema.org classes in the validation.\n",
    "\n",
    "Note that the DDE intends to make schemas more human interpretable; hence, it does NOT like excessive nesting and circularity in the `$validation` portion of the jsonschema. While infinite, looping, references are allowed in schema.org, it can cause errors if enforced in the `$validation` portion. To bypass this, extensive nesting has been avoided by including simplified classes as objects in the definitions portion of the `$validation`. Although these simplified objects may not include ALL the properties allowable for that class, it should not raise errors if they are included during validation either.   \n",
    "\n",
    "Already in place:\n",
    "* If a property comes from bioschemas, it will need to be defined in the @graph\n",
    "* If a property exists in the hierarchy and can be inherited, then it should not need to be defined. Instead, it should be defined in the `$validation`\n",
    "\n",
    "If a property comes from schema.org it may or may not be need to be defined. This depends on whether or not the property exists in the hierarchy from which this class is derived. This is because the only real constraints provided by schema.org are class hierarchies and property<->class relationships. Hence, the DDE only allows the inheritance of properties that are within the class hierarchy. Marginality, cardinality, and other useful constraints (eg- ontologies) in the biomedical research space come from bioschemas.\n",
    "\n",
    "Not yet in place:\n",
    "* If a schema.org property does not exist in the hierarchy, it normally does not apply to this class,\n",
    "* If this is the case, it should be created with the \"sameAs\" property\n",
    "\n",
    "#### To Do:\n",
    "Some of the yaml files are throwing errors the following errors:\n",
    "`ScannerError: mapping values are not allowed here`\n",
    "Currently, we log the error and ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import pathlib\n",
    "import yaml\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "tmp_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(tmp_dir)\n",
    "available = os.listdir(parent_dir)\n",
    "outputdirectory = os.path.join(parent_dir,'specifications')\n",
    "inputdirectory = os.path.join(parent_dir,'Bioschemas-Validator')\n",
    "input_profiles = os.path.join(inputdirectory,'profile_json')\n",
    "input_marginality = os.path.join(inputdirectory,'profile_marginality')\n",
    "input_yml = os.path.join(inputdirectory,'profile_yml')\n",
    "\n",
    "specifications = os.listdir(input_profiles)\n",
    "datapath = os.path.join('results','resulting json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the yaml file\n",
    "def get_yml_dict(theymlfile):\n",
    "    try:\n",
    "        with open(theymlfile,'r',encoding=\"utf8\") as ymlin:\n",
    "            tmpyml = yaml.load_all(ymlin, Loader=yaml.FullLoader)\n",
    "            for eachyml in tmpyml:\n",
    "                if '<!DOCTYPE HTML>' in eachyml:\n",
    "                    break\n",
    "                ymldict = eachyml\n",
    "    except:\n",
    "        with open(theymlfile,'r',encoding=\"latin1\") as ymlin:\n",
    "            tmpyml = yaml.load_all(ymlin, Loader=yaml.FullLoader)\n",
    "            for eachyml in tmpyml:\n",
    "                if '<!DOCTYPE HTML>' in eachyml:\n",
    "                    break\n",
    "                ymldict = eachyml\n",
    "    return(ymldict)\n",
    "\n",
    "\n",
    "def test_yml_mapping(ymldict):\n",
    "    if 'mapping' in ymldict.keys():\n",
    "        mapping=True\n",
    "    else:\n",
    "        mapping=ymldict.keys()\n",
    "    return(mapping)\n",
    "\n",
    "\n",
    "#### Create the base class\n",
    "def create_new_dict():\n",
    "    newdict = {}\n",
    "    newdict['@context'] = {\n",
    "        \"schema\": \"http://schema.org/\",\n",
    "        \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "        \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "        \"bioschemas\": \"http://discovery.biothings.io/view/bioschemas/\"\n",
    "      } \n",
    "    return(newdict)\n",
    "\n",
    "\n",
    "def get_schema_base():\n",
    "    ## The DDE does not appear to be using schema version 13. Since the version is not know, we'll use 12 for now\n",
    "    schemabase = 'https://raw.githubusercontent.com/schemaorg/schemaorg/main/data/releases/12.0/schemaorg-all-http.jsonld'\n",
    "    r = requests.get(schemabase)\n",
    "    schematree = r.text\n",
    "    return(schematree)\n",
    "\n",
    "\n",
    "def grab_class_info(schematree,eachversion,ymldict):\n",
    "    parentclass = ymldict['hierarchy'][-1]\n",
    "    if parentclass in schematree:\n",
    "        parenttype = \"schema:\"\n",
    "    else:\n",
    "        parenttype = \"bioschemas:\"\n",
    "    try:\n",
    "        description = ymldict['spec_info']['subtitle']+\" \"+ymldict['spec_info']['description']+\" Version: \"+ymldict['spec_info']['version']\n",
    "    except:\n",
    "        description = ymldict['spec_info']['subtitle']+\" \"+\" Version: \"+ymldict['spec_info']['version']\n",
    "    classinfo = {\n",
    "      \"@id\": \"bioschemas:\"+ymldict['name'],\n",
    "      \"@type\": \"rdfs:Class\",\n",
    "      \"rdfs:comment\": description,\n",
    "      \"schema:schemaVersion\": [\"https://bioschemas.org/\"+ymldict['spec_type'].lower()+\"s/\"+ymldict['name']+\"/\"+eachversion.replace(\".json\",\"\")],  \n",
    "      \"rdfs:label\": ymldict['name'],\n",
    "      \"rdfs:subClassOf\": {\n",
    "        \"@id\": parenttype+parentclass\n",
    "      }\n",
    "    }\n",
    "    return(classinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create the validation for the class\n",
    "\n",
    "def load_dictionaries():\n",
    "    from dde_reusable_objects import expected_type_dict\n",
    "    from dde_reusable_objects import reusable_definitions\n",
    "    return(expected_type_dict, reusable_definitions)\n",
    "\n",
    "\n",
    "def generate_type(expected_type_dict, propertytype):\n",
    "    if propertytype in expected_type_dict.keys():\n",
    "        matched_type = expected_type_dict[propertytype]\n",
    "    else:\n",
    "        matched_type = False\n",
    "    return(matched_type)\n",
    "\n",
    "\n",
    "def generate_base_dict(expectedtype):\n",
    "    base_dict = {\n",
    "        \"@type\": expectedtype,\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"name\": {\n",
    "            \"type\": \"string\"  \n",
    "          }  \n",
    "        },\n",
    "        \"required\": []\n",
    "    }\n",
    "    return(base_dict)\n",
    "\n",
    "\n",
    "def import_reusable_objects(reusable_definitions,rangelist):\n",
    "    expected_types = [x[\"@id\"].replace(\"bioschemas:\",\"\") for x in rangelist]\n",
    "    all_expected_types = [x.replace(\"schema:\",\"\") for x in expected_types]\n",
    "    definitionslist = [x for x in all_expected_types if x.lower() in reusable_definitions.keys()]\n",
    "    definitiondict = {}\n",
    "    referencelist = {}\n",
    "    for eachdefinition in definitionslist:\n",
    "        definitiondict[eachdefinition.lower()] = reusable_definitions[eachdefinition.lower()]\n",
    "        referencelist[eachdefinition.lower()] = {\"$ref\":\"#/definitions/\"+eachdefinition.lower()}        \n",
    "    return(definitiondict,referencelist)\n",
    "\n",
    "\n",
    "def check_type(expected_type_dict, referencelist, definitiondict, propertytype_info):\n",
    "    referencename = propertytype_info.replace(\"bioschemas:\",\"\").replace(\"schema:\",\"\").lower()\n",
    "    matched_type = generate_type(expected_type_dict, propertytype_info)\n",
    "    if matched_type != False:\n",
    "        actualtype = matched_type\n",
    "    else:\n",
    "        if referencename in referencelist.keys():\n",
    "            actualtype = referencelist[referencename]\n",
    "        else:\n",
    "            ### create reference and property\n",
    "            actualtype = {\"$ref\":\"#/definitions/\"+referencename}\n",
    "            referencelist[referencename] = actualtype\n",
    "            definitiondict[referencename] = generate_base_dict(propertytype_info)    \n",
    "    return(actualtype, referencelist, definitiondict)\n",
    "\n",
    "\n",
    "def cardinality_check(expected_type_dict, referencelist, definitiondict, eachproperty):\n",
    "    rangelist = get_rangelist(eachproperty)\n",
    "    ## Generate the base object\n",
    "    if \"bsc_description\" in eachproperty.keys():\n",
    "        valpropdict = {\"description\":eachproperty['bsc_description']+\" \"+eachproperty['description']}\n",
    "    else:\n",
    "        valpropdict = {\"description\":eachproperty['description']}\n",
    "    ## Check cardinality    \n",
    "    if eachproperty['cardinality'] != \"MANY\": ## There can only be one expected value or cardinality not defined\n",
    "        ## Check number of expected types\n",
    "        if len(rangelist) == 1: ## There can only be one expected type\n",
    "            propertytype = rangelist[0]\n",
    "            actualtype, referencelist, definitiondict = check_type(expected_type_dict, referencelist, definitiondict, propertytype['@id'])\n",
    "            valpropdict.update(actualtype)\n",
    "        else: ## There are more than one expected type\n",
    "            propertyelements = []\n",
    "            for propertytype in rangelist:\n",
    "                actualtype, referencelist, definitiondict = check_type(expected_type_dict, referencelist, definitiondict, propertytype['@id'])\n",
    "                if actualtype not in propertyelements:\n",
    "                    propertyelements.append(actualtype)\n",
    "            valpropdict[\"oneOf\"] = propertyelements\n",
    "                \n",
    "    else: ## each property can have many elements, ie- Cardinality == Many\n",
    "        ## Check number of expected types\n",
    "        if len(rangelist) == 1: ## If only one type expected, but many of it are allowed\n",
    "            propertytype = rangelist[0]\n",
    "            actualtype, referencelist, definitiondict = check_type(expected_type_dict, referencelist, definitiondict, propertytype['@id'])\n",
    "            valpropdict[\"oneOf\"] = [\n",
    "                  actualtype,\n",
    "                  {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": actualtype\n",
    "                  }\n",
    "              ]  \n",
    "        else: ## Many types are allowed, and many values are expected\n",
    "            propertyelements = []\n",
    "            for propertytype in rangelist:\n",
    "                actualtype, referencelist, definitiondict = check_type(expected_type_dict, referencelist, definitiondict, propertytype['@id'])\n",
    "                if actualtype not in propertyelements:\n",
    "                    propertyelements.append(actualtype)\n",
    "                manyactualtype = {\"type\":\"array\", \"items\":actualtype}\n",
    "                if manyactualtype not in propertyelements:\n",
    "                    propertyelements.append(manyactualtype)\n",
    "            valpropdict[\"anyOf\"] = propertyelements            \n",
    "    return(valpropdict, referencelist, definitiondict)\n",
    "\n",
    "\n",
    "#### Generate validation content\n",
    "\n",
    "def generate_validation(ymldict):\n",
    "    expected_type_dict, reusable_definitions = load_dictionaries()\n",
    "    propertylist = ymldict['mapping']\n",
    "    validationdict = {\n",
    "      \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\":{},\n",
    "      \"required\":[],\n",
    "      \"recommended\":[],\n",
    "      \"optional\":[],\n",
    "      \"definitions\":{}\n",
    "    }\n",
    "    for eachproperty in propertylist:\n",
    "        rangelist = get_rangelist(eachproperty)\n",
    "        definitiondict,referencelist = import_reusable_objects(reusable_definitions,rangelist)\n",
    "        valpropdict, referencelist, definitiondict = cardinality_check(expected_type_dict, referencelist, definitiondict, eachproperty)\n",
    "        actualname, property_source = split_property_name(eachproperty[\"property\"])\n",
    "        validationdict[\"properties\"][actualname]=valpropdict\n",
    "        validationdict['definitions'].update(definitiondict)\n",
    "        #### Include categorycode if propertyvalue is used\n",
    "        if \"propertyvalue\" in validationdict['definitions'].keys():\n",
    "            validationdict['definitions']['categorycode']=reusable_definitions[\"categorycode\"]\n",
    "        #### Include definedTermSet if definedTerm is used\n",
    "        if \"definedterm\" in validationdict['definitions'].keys():\n",
    "            validationdict['definitions']['definedtermset']=reusable_definitions[\"definedtermset\"]\n",
    "        #### Include creativework if person is used (but only if it isn't already included)\n",
    "        if \"person\" in validationdict['definitions'].keys():\n",
    "            if \"creativework\" not in validationdict['definitions'].keys():\n",
    "                validationdict['definitions']['creativework']=reusable_definitions['creativework']\n",
    "        try:\n",
    "            marginality = eachproperty[\"marginality\"]\n",
    "        except:\n",
    "            marginality = None\n",
    "        if marginality == \"Minimum\":\n",
    "            validationdict['required'].append(actualname)\n",
    "        elif marginality == \"Recommended\":\n",
    "            validationdict['recommended'].append(actualname)\n",
    "        elif marginality == \"Optional\":\n",
    "            validationdict['optional'].append(actualname)\n",
    "    return(validationdict)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define properties for the graph\n",
    "\n",
    "def check_property_source(eachproperty): \n",
    "    if eachproperty['type']==\"bioschemas\":\n",
    "        namespace = \"bioschemas\"\n",
    "    elif eachproperty['type']==\"external\":\n",
    "        namespace = \"TBD\"\n",
    "    else:\n",
    "        namespace = \"schema\"\n",
    "    return(namespace)\n",
    "\n",
    "def get_rangelist(eachproperty):\n",
    "    typelist = eachproperty['expected_types']\n",
    "    bioschemalist = [x for x in typelist if x in specifications]\n",
    "    expected_bioschemas = [\"bioschemas:\"+x for x in bioschemalist]\n",
    "    expected_schemas = [\"schema:\"+x for x in typelist if x not in bioschemalist]\n",
    "    rangelist = [{\"@id\":x} for x in expected_bioschemas]\n",
    "    for x in expected_schemas:\n",
    "        rangelist.append({\"@id\":x})\n",
    "    return(rangelist)\n",
    "\n",
    "def get_domain(eachspec):\n",
    "    domain = {\"@id\":\"bioschemas:\"+eachspec}\n",
    "    return(domain)\n",
    "\n",
    "\n",
    "def split_property_name(propertyname):\n",
    "    try:\n",
    "        propertynameinfo = propertyname.split(\":\")\n",
    "        property_source = propertynameinfo[0]\n",
    "        actualname = propertynameinfo[1]\n",
    "    except:\n",
    "        actualname = propertyname\n",
    "        property_source = None\n",
    "    return(actualname, property_source)\n",
    "\n",
    "\n",
    "def create_bioschema_property(eachspec, eachproperty):\n",
    "    domain = get_domain(eachspec)\n",
    "    rangelist = get_rangelist(eachproperty)\n",
    "    namespace = check_property_source(eachproperty)\n",
    "    source4context = False\n",
    "    if namespace==\"bioschemas\":\n",
    "        ### Create property\n",
    "        try:\n",
    "            description = eachproperty[\"description\"]+\" \"+eachproperty['bsc_description']\n",
    "        except:\n",
    "            description = eachproperty[\"description\"]\n",
    "        property_dict = {\n",
    "            \"@id\": \"bioschemas:\"+eachproperty['property'],\n",
    "            \"rdfs:comment\": description,\n",
    "            \"@type\": \"rdf:Property\",\n",
    "            \"rdfs:label\": eachproperty['property'],\n",
    "            \"schema:domainIncludes\": domain,\n",
    "            \"schema:rangeIncludes\": rangelist\n",
    "        }\n",
    "    elif namespace==\"TBD\":\n",
    "        ### Create externally referencing property\n",
    "        try:\n",
    "            description = eachproperty[\"description\"]+\" \"+eachproperty['bsc_description']\n",
    "        except:\n",
    "            description = eachproperty[\"description\"]\n",
    "        actualname, propertysource = split_property_name(eachproperty['property'])\n",
    "        property_dict = {\n",
    "            \"@id\": eachproperty['property'],\n",
    "            \"rdfs:comment\": description,\n",
    "            \"@type\": \"rdf:Property\",\n",
    "            \"rdfs:label\": actualname,\n",
    "            \"schema:domainIncludes\": domain,\n",
    "            \"schema:rangeIncludes\": rangelist\n",
    "        }\n",
    "        if propertysource != None:\n",
    "            source4context = {propertysource:eachproperty[\"type_url\"].replace(actualname,'')}\n",
    "    else:\n",
    "        property_dict = False\n",
    "    return(property_dict,source4context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Assemble the jsonld file\n",
    "\n",
    "def parse_spec_version(tmpinputyml,eachversion,eachspec,specifications):\n",
    "    theymlfile = os.path.join(tmpinputyml,eachversion.replace('.json','.html'))\n",
    "    graphlist = []\n",
    "    tmpdict = create_new_dict()\n",
    "    ymldict = get_yml_dict(theymlfile)\n",
    "    schematree = get_schema_base()\n",
    "    classinfo = grab_class_info(schematree,eachversion,ymldict)\n",
    "    expected_type_dict, reusable_definitions = load_dictionaries()\n",
    "    mapping = test_yml_mapping(ymldict)\n",
    "    if mapping == True:\n",
    "        propertylist = ymldict['mapping']\n",
    "        validationdict = generate_validation(ymldict)\n",
    "        classinfo['$validation']=validationdict\n",
    "        graphlist.append(classinfo)\n",
    "        for eachproperty in propertylist:\n",
    "            bioschemaprop,source4context = create_bioschema_property(eachspec, eachproperty)\n",
    "            if bioschemaprop != False:\n",
    "                graphlist.append(bioschemaprop)\n",
    "            if source4context != False:\n",
    "                tmpdict['@context'].update(source4context)\n",
    "        tmpdict['@graph']=graphlist\n",
    "    else:\n",
    "        print(mapping)\n",
    "        tmpdict=False\n",
    "    return(tmpdict)\n",
    "\n",
    "\n",
    "#### Parse specifications\n",
    "def parse_for_dde(input_profiles,datapath,test=False):\n",
    "    specifications = os.listdir(input_profiles)\n",
    "    failures = []\n",
    "    if test==True:\n",
    "        eachspec = specifications[-4] ##TaxonRank (-3), LabProtocol (14), Course (5)\n",
    "        tmpinputprofilepath = os.path.join(input_profiles,eachspec)\n",
    "        tmpinputyml = os.path.join(input_yml,eachspec)\n",
    "        spec_profs = os.listdir(tmpinputprofilepath)\n",
    "        eachversion = spec_profs[-1]\n",
    "        tmpdict = parse_spec_version(tmpinputyml,eachversion,eachspec,specifications)\n",
    "        if os.path.exists(os.path.join(datapath,str(eachspec)))==False:\n",
    "            os.makedirs(os.path.join(datapath,str(eachspec)))\n",
    "        outputpath = os.path.join(datapath,str(eachspec))\n",
    "        with open(os.path.join(outputpath,str(eachspec)+'_v'+str(eachversion)),\"w+\") as outfile:\n",
    "            outfile.write(json.dumps(tmpdict, indent=4, sort_keys=False))\n",
    "    else:\n",
    "        for eachspec in specifications:\n",
    "            tmpinputprofilepath = os.path.join(input_profiles,eachspec)\n",
    "            tmpinputyml = os.path.join(input_yml,eachspec)\n",
    "            spec_profs = os.listdir(tmpinputprofilepath)\n",
    "            for eachversion in spec_profs:\n",
    "                try:\n",
    "                    tmpdict = parse_spec_version(tmpinputyml,eachversion,eachspec,specifications)\n",
    "                except:\n",
    "                    tmpdict=False\n",
    "                    failures.append(\"error parsing yml for: \"+str(eachspec)+'_v'+str(eachversion))\n",
    "                if tmpdict==False:\n",
    "                    print(\"The specification: \",str(eachspec)+'_v'+str(eachversion),' does not have a mapping in the yaml')\n",
    "                    failures.append(\"no mapping in yml for: \"+str(eachspec)+'_v'+str(eachversion))\n",
    "                else:\n",
    "                    if os.path.exists(os.path.join(datapath,str(eachspec)))==False:\n",
    "                        os.makedirs(os.path.join(datapath,str(eachspec)))\n",
    "                    outputpath = os.path.join(datapath,str(eachspec))\n",
    "                    with open(os.path.join(outputpath,str(eachspec)+'_v'+str(eachversion)),\"w+\") as outfile:\n",
    "                        outfile.write(json.dumps(tmpdict, indent=4, sort_keys=False))\n",
    "        with open(os.path.join(datapath,'failures.txt'),'w+') as failurelog:\n",
    "            for eachitem in failures:\n",
    "                failurelog.write(eachitem+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Beacon\n",
      "1 BioSample\n",
      "2 ChemicalSubstance\n",
      "3 ComputationalTool\n",
      "4 ComputationalWorkflow\n",
      "5 Course\n",
      "6 CourseInstance\n",
      "7 DataCatalog\n",
      "8 DataRecord\n",
      "9 Dataset\n",
      "10 Event\n",
      "11 FormalParameter\n",
      "12 Gene\n",
      "13 Journal\n",
      "14 LabProtocol\n",
      "15 MolecularEntity\n",
      "16 Organization\n",
      "17 Person\n",
      "18 Phenotype\n",
      "19 Protein\n",
      "20 ProteinAnnotation\n",
      "21 ProteinStructure\n",
      "22 PublicationIssue\n",
      "23 PublicationVolume\n",
      "24 RNA\n",
      "25 Sample\n",
      "26 ScholarlyArticle\n",
      "27 SemanticTextAnnotation\n",
      "28 Study\n",
      "29 Taxon\n",
      "30 TaxonName\n",
      "31 Tool\n",
      "32 TrainingMaterial\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i < len(specifications):\n",
    "    print(i, specifications[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['property', 'expected_types', 'description', 'type', 'type_url', 'bsc_description', 'marginality', 'cardinality', 'controlled_vocab', 'example'])\n",
      "author\n",
      "dict_keys(['property', 'expected_types', 'description', 'type', 'type_url', 'bsc_description', 'marginality', 'cardinality', 'controlled_vocab', 'example'])\n"
     ]
    }
   ],
   "source": [
    "## Inspect a dictionary parsed from the yaml file\n",
    "#print(specifications)\n",
    "eachspec = specifications[14] ##TaxonRank (-3), LabProtocol (14), Course (5)\n",
    "tmpinputprofilepath = os.path.join(input_profiles,eachspec)\n",
    "tmpinputmarginpath = os.path.join(input_marginality,eachspec)\n",
    "tmpinputyml = os.path.join(input_yml,eachspec)\n",
    "spec_profs = os.listdir(tmpinputprofilepath)\n",
    "eachversion = spec_profs[-1]\n",
    "thejsonfile = os.path.join(tmpinputprofilepath,eachversion)\n",
    "themarginfile = os.path.join(tmpinputmarginpath,eachversion)\n",
    "theymlfile = os.path.join(tmpinputyml,eachversion.replace('.json','.html'))\n",
    "#print(os.listdir(tmpinputyml))\n",
    "#print(theymlfile)\n",
    "\n",
    "ymldict = get_yml_dict(theymlfile)\n",
    "#print(ymldict.keys())\n",
    "i = 0\n",
    "print(ymldict['mapping'][0].keys())\n",
    "print(ymldict['mapping'][0]['property'])\n",
    "print(ymldict['mapping'][1].keys())\n",
    "#mapping = test_yml_mapping(ymldict)\n",
    "#print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a test run\n",
    "parse_for_dde(input_profiles,datapath,test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'previous_version', 'previous_release', 'status', 'spec_type', 'group', 'use_cases_url', 'cross_walk_url', 'gh_tasks', 'live_deploy', 'parent_type', 'hierarchy', 'spec_info'])\n",
      "The specification:  Journal_v0.1-DRAFT-2019_01_29.json  does not have a mapping in the yaml\n",
      "The specification:  Organization_v0.2-DRAFT-2019_07_17.json  does not have a mapping in the yaml\n",
      "The specification:  Sample_v0.1-DRAFT-2018_02_25.json  does not have a mapping in the yaml\n",
      "The specification:  Sample_v0.2-DRAFT-2018_11_09.json  does not have a mapping in the yaml\n",
      "The specification:  Sample_v0.2-DRAFT-2018_11_10.json  does not have a mapping in the yaml\n",
      "The specification:  Sample_v0.2-RELEASE-2018_11_10.json  does not have a mapping in the yaml\n",
      "The specification:  Tool_v0.3-DRAFT-2018_11_21.json  does not have a mapping in the yaml\n",
      "The specification:  Tool_v0.3-DRAFT-2019_07_18.json  does not have a mapping in the yaml\n",
      "The specification:  TrainingMaterial_v0.7-DRAFT-2019_11_08.json  does not have a mapping in the yaml\n"
     ]
    }
   ],
   "source": [
    "## Run through all specifications\n",
    "parse_for_dde(input_profiles,datapath,test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating list of most common properties and expected types\n",
    "\n",
    "The DDE is undergoing many improvements and getting refactored on the back-end. To ensure that the DDE can be used by as many people as possible with little/no knowledge of the JSON schema, the most common bioschemas properties and their expected types are being evaluated for inclusions as default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join('results','frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_illegal_chars(theymlfile):\n",
    "    with open(theymlfile,'r',encoding=\"utf8\") as ymlin:\n",
    "        theymlfile.replace(\"#x0080\",\"\\#x0080\")\n",
    "    return(theymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "  specification    property  expected_type  marginality\n",
      "0        Beacon  aggregator      [Boolean]  Recommended\n",
      "1        Beacon     dataset  [DataCatalog]      Minimum\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Get frequency of expected types\n",
    "allproplist = []\n",
    "for eachspec in specifications:\n",
    "    tmpinputprofilepath = os.path.join(input_profiles,eachspec)\n",
    "    tmpinputmarginpath = os.path.join(input_marginality,eachspec)\n",
    "    tmpinputyml = os.path.join(input_yml,eachspec)\n",
    "    spec_profs = os.listdir(tmpinputprofilepath)\n",
    "    eachversion = spec_profs[-1]\n",
    "    thejsonfile = os.path.join(tmpinputprofilepath,eachversion)\n",
    "    themarginfile = os.path.join(tmpinputmarginpath,eachversion)\n",
    "    theymlfile = os.path.join(tmpinputyml,eachversion.replace('.json','.html'))\n",
    "    try:\n",
    "        ymldict = get_yml_dict(theymlfile)\n",
    "    except:\n",
    "        ymlin = clean_illegal_chars(theymlfile)\n",
    "        tmpyml = yaml.load_all(ymlin, Loader=yaml.FullLoader)\n",
    "        for eachyml in tmpyml:\n",
    "            if '<!DOCTYPE HTML>' in eachyml:\n",
    "                break\n",
    "            ymldict = eachyml\n",
    "    try:\n",
    "        mappingdict = ymldict['mapping']\n",
    "        for eachdict in mappingdict:\n",
    "            tmpdict = {'specification':eachspec,'property':eachdict['property'],'expected_type':eachdict['expected_types'],'marginality':eachdict['marginality']}\n",
    "            allproplist.append(tmpdict)\n",
    "    except:\n",
    "        print(eachspec)\n",
    "\n",
    "allpropdf = pandas.DataFrame(allproplist)\n",
    "print(allpropdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get frequency of combinations of expected_type\n",
    "allpropdf['expected_types_raw'] = allpropdf['expected_type'].astype(str)\n",
    "prop_freq = allpropdf.groupby(['property','expected_types_raw']).size().reset_index(name='counts')\n",
    "combi_exp_freq = allpropdf.groupby('expected_types_raw').size().reset_index(name='counts')\n",
    "\n",
    "prop_freq.sort_values('counts',ascending=False,inplace=True)\n",
    "combi_exp_freq.sort_values('counts',ascending=False,inplace=True)\n",
    "\n",
    "prop_freq.to_csv(os.path.join(datapath,'property_frequency.tsv'),sep='\\t',header=True)\n",
    "combi_exp_freq.to_csv(os.path.join(datapath,'raw_expected_types_frequency.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         expected_type  counts\n",
      "74                Text     262\n",
      "77                 URL     237\n",
      "60       PropertyValue      71\n",
      "19        CreativeWork      66\n",
      "27         DefinedTerm      54\n",
      "..                 ...     ...\n",
      "38        HowToSection       1\n",
      "1   AdministrativeArea       1\n",
      "46       MedicalEntity       1\n",
      "47     MolecularEntity       1\n",
      "78     VirtualLocation       1\n",
      "\n",
      "[79 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## Get frequency of base expected type\n",
    "expected_types = allpropdf.explode('expected_type')\n",
    "expected_freq = expected_types.groupby('expected_type').size().reset_index(name='counts')\n",
    "expected_freq.sort_values('counts',ascending=False,inplace=True)\n",
    "expected_freq.to_csv(os.path.join(datapath,'expected_types_frequency.tsv'),sep='\\t',header=True)\n",
    "print(expected_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_object(schematype):\n",
    "    basedict = {\"@type\": schematype,\"type\": \"object\",\"properties\":{},\"required\":[]}\n",
    "    return basedict\n",
    "\n",
    "def replace_json_validation(expected_dict):\n",
    "    expected_dict[\"text\"]={\"type\":\"string\"}\n",
    "    expected_dict[\"url\"]={\"type\":\"string\",\"format\":\"uri\"}\n",
    "    expected_dict[\"number\"] = {\"type\":\"number\"}\n",
    "    expected_dict[\"integer\"] = {\"type\":\"integer\"}\n",
    "    expected_dict[\"boolean\"] = {\"type\":\"boolean\"}\n",
    "    expected_dict[\"date\"]={\"type\":\"string\",\"format\":\"date\"}\n",
    "    return expected_dict\n",
    "\n",
    "def translate(typestring):\n",
    "    if typestring == \"Text\":\n",
    "        newtype = {\"type\":\"string\"}\n",
    "    elif typestring == \"URL\":\n",
    "        newtype = {\"type\":\"string\",\"format\":\"uri\"}\n",
    "    elif typestring == \"Number\":\n",
    "        newtype = {\"type\":\"number\"}\n",
    "    elif typestring == \"Integer\":\n",
    "        newtype = {\"type\":\"integer\"}\n",
    "    elif typestring == \"Boolean\":\n",
    "        newtype = {\"type\":\"boolean\"}\n",
    "    elif typestring == \"Date\":\n",
    "        newtype = {\"type\":\"string\",\"format\":\"date\"}\n",
    "    else:\n",
    "        newtype = {\"type\":\"object\",\"@type\":typestring}\n",
    "    return newtype\n",
    "\n",
    "def generate_base_bs_rules(allpropdf):\n",
    "    ## Get required properties for each class\n",
    "    required_props = allpropdf.loc[allpropdf['marginality']==\"Minimum\"].copy()\n",
    "    required_props.to_csv(os.path.join(datapath,'required_props.tsv'),sep='\\t',header=True)\n",
    "    required_props['expected_count'] = required_props.apply(lambda row: count_expected_types(row['expected_type']), axis=1)\n",
    "    required_props['single_expected'] = [x for x in required_props['expected_type']]\n",
    "    required_props = required_props.explode('single_expected')\n",
    "    bioschema_list = required_props['specification'].unique().tolist()\n",
    "    validation_dict = {}\n",
    "    for eachspec in bioschema_list:\n",
    "        tmpdf = required_props.loc[required_props['specification']==eachspec]\n",
    "        property_list = tmpdf['property'].unique().tolist()\n",
    "        property_dict = {}\n",
    "        for eachprop in property_list:\n",
    "            propdf = tmpdf.loc[tmpdf['property']==eachprop].copy()\n",
    "            if len(propdf)<2:\n",
    "                ## property has a single expected type\n",
    "                property_dict[eachprop] = translate(propdf.iloc[0]['single_expected'])\n",
    "            if len(propdf)>1:\n",
    "                propdf['translated'] = propdf.apply(lambda row: translate(row['single_expected']), axis=1)\n",
    "                property_dict[eachprop] = {\"oneOf\": propdf['translated'].tolist()}\n",
    "        validation_dict[eachspec.lower()] = {\"@type\":eachspec,\"type\":\"object\",\"properties\":property_dict,\"required\":property_list}\n",
    "    return validation_dict\n",
    "\n",
    "def count_expected_types(expected_type):\n",
    "    type_count = len(expected_type)\n",
    "    return type_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert base expected type into json dictionary for expected types\n",
    "validation_dict = generate_base_bs_rules(allpropdf)\n",
    "\n",
    "expected_objects = pandas.read_csv(os.path.join(datapath,'expected_types_frequency.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "\n",
    "expected_dict = {}\n",
    "for i in range(len(expected_objects)):\n",
    "    schematype = expected_objects.iloc[i]['expected_type']\n",
    "    expected_dict[schematype.lower()]=create_base_object(schematype)\n",
    "\n",
    "expected_dict = replace_json_validation(expected_dict)\n",
    "expected_dict.update(validation_dict)\n",
    "\n",
    "with open(os.path.join(datapath,'base_types.json'),'w') as outfile:\n",
    "    outfile.write(json.dumps(expected_dict, indent=2, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@type': 'Beacon', 'type': 'object', 'properties': {'dataset': {'type': 'object', '@type': 'DataCatalog'}, 'name': {'type': 'string'}, 'potentialAction': {'type': 'object', '@type': 'Action'}, 'provider': {'oneOf': [{'type': 'object', '@type': 'Organization'}, {'type': 'object', '@type': 'Person'}]}, 'rdf:type': {'type': 'string', 'format': 'uri'}, 'supportedRefs': {'type': 'string'}, 'url': {'type': 'string', 'format': 'uri'}}, 'required': ['dataset', 'name', 'potentialAction', 'provider', 'rdf:type', 'supportedRefs', 'url']}\n"
     ]
    }
   ],
   "source": [
    "print(expected_dict['beacon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining rules for a validation builder\n",
    "\n",
    "This section is just to test the logic for creating combined JSON schema validation rules\n",
    "For example: how to convert one or many rules for single or multiple types of json validation rules\n",
    "\n",
    "Example cases:\n",
    "* ONE of a single rule type: ONE type: Text (string)\n",
    "* Many of a single rule type: Many of type: Text (string)\n",
    "* One of two potential rule types: Text (string) or Person (object)\n",
    "* One of two potential rule types: Text (string) or URL (Formatted Text)\n",
    "* Many of two potential rule types: Text (string) or Person (object)\n",
    "* Many of two potential rules types: Text (string) or URL (formatted string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_rule(expected_dict,rule_key):\n",
    "    rule_dict = expected_dict[rule_key]\n",
    "    return rule_dict\n",
    "\n",
    "def get_one_many_rule(expected_dict,rule_list):\n",
    "    if len(set(rule_list))==1:\n",
    "        ## This is a list of a single rule, treat as such\n",
    "        rule_dict = get_single_rule(expected_dict,rule_list[0])\n",
    "    else:\n",
    "        rule_dict = {}\n",
    "        rule_val_list = []\n",
    "        for each_rule in rule_list:\n",
    "            rule_val_list.append(expected_dict[each_rule])\n",
    "        rule_dict[\"oneOf\"] = rule_val_list\n",
    "    return rule_dict\n",
    "\n",
    "def get_many_single_rule(expected_dict,rule_key):\n",
    "    rule_dict = {}\n",
    "    rule_val_list = []\n",
    "    rule_val_list.append(expected_dict[rule_key])\n",
    "    rule_val_list.append({\"type\":\"array\",\"items\":expected_dict[rule_key]})\n",
    "    rule_dict[\"oneOf\"] = rule_val_list\n",
    "    return rule_dict\n",
    "\n",
    "def get_many_many_rules(expected_dict,rule_list):\n",
    "    rule_dict = {}\n",
    "    ### check if they are all of the same json schema types\n",
    "    rule_set = set(rule_list)\n",
    "    if len(rule_set) == 1 and len(rule_list) == 1:\n",
    "        ## This is actually just a single rule placed in a list\n",
    "        rule_dict = get_many_single_rule(expected_dict,rule_list[0])\n",
    "    \n",
    "    elif len(rule_set) == 1 and len(rule_list) > 1:\n",
    "        ## This is actually just a multiples of a single rule in a list, treat as above\n",
    "        rule_dict = get_many_single_rule(expected_dict,rule_list[0])\n",
    "        \n",
    "    elif len(rule_set)> 1:\n",
    "        ## The options are mixed between types, use \"anyOf\" for the array\n",
    "        rule_val_list = []\n",
    "        for each_rule in rule_list:\n",
    "            rule_val_list.append(expected_dict[each_rule])\n",
    "            rule_val_list.append({\"type\":\"array\",\"items\":expected_dict[each_rule]})\n",
    "        rule_dict[\"anyOf\"] = rule_val_list\n",
    "        \n",
    "    return rule_dict\n",
    "\n",
    "\n",
    "def get_rules(expected_dict,rule_list,cardinality=\"one\"):\n",
    "    if isinstance(rule_list,str) == True:\n",
    "        if cardinality.lower() == \"one\":\n",
    "            rule_dict = get_single_rule(expected_dict,rule_list)\n",
    "        if cardinality.lower() == \"many\":\n",
    "            rule_dict = get_many_single_rule(expected_dict,rule_list)\n",
    "    if isinstance(rule_list,list):\n",
    "        if cardinality.lower() == \"one\":\n",
    "            rule_dict = get_one_many_rule(expected_dict,rule_list)\n",
    "        if cardinality.lower() == \"many\":\n",
    "            rule_dict = get_many_many_rules(expected_dict,rule_list)\n",
    "    return rule_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the above functions\n",
    "cardinality = \"one\"\n",
    "\n",
    "with open(os.path.join(datapath,'base_types.json'),'r') as infile:\n",
    "    expected_dict = json.load(infile, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'string'} \n",
      "\n",
      "{'oneOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}]} \n",
      "\n",
      "{'oneOf': [{'type': 'string'}, {'@type': 'Person', 'type': 'object', 'properties': {'description': {'type': 'string'}, 'mainEntityOfPage': {'oneOf': [{'type': 'object', '@type': 'CreativeWork'}, {'type': 'string', 'format': 'uri'}]}, 'name': {'type': 'string'}}, 'required': ['description', 'mainEntityOfPage', 'name']}]} \n",
      "\n",
      "{'oneOf': [{'type': 'string'}, {'type': 'string', 'format': 'uri'}]} \n",
      "\n",
      "{'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}, {'@type': 'Person', 'type': 'object', 'properties': {'description': {'type': 'string'}, 'mainEntityOfPage': {'oneOf': [{'type': 'object', '@type': 'CreativeWork'}, {'type': 'string', 'format': 'uri'}]}, 'name': {'type': 'string'}}, 'required': ['description', 'mainEntityOfPage', 'name']}, {'type': 'array', 'items': {'@type': 'Person', 'type': 'object', 'properties': {'description': {'type': 'string'}, 'mainEntityOfPage': {'oneOf': [{'type': 'object', '@type': 'CreativeWork'}, {'type': 'string', 'format': 'uri'}]}, 'name': {'type': 'string'}}, 'required': ['description', 'mainEntityOfPage', 'name']}}]} \n",
      "\n",
      "{'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'type': 'string'}}, {'type': 'string', 'format': 'uri'}, {'type': 'array', 'items': {'type': 'string', 'format': 'uri'}}]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ONE of a single rule type: ONE type: Text (string)\n",
    "cardinality = \"One\"\n",
    "rule_dict = get_rules(expected_dict,\"text\",cardinality)\n",
    "print(rule_dict,'\\n')\n",
    "\n",
    "## Many of a single rule type: Many of type: Text (string)\n",
    "cardinality = \"MANY\"\n",
    "rule_dict = get_rules(expected_dict,\"text\",cardinality)\n",
    "print(rule_dict,'\\n')\n",
    "\n",
    "## One of two potential rule types: Text (string) or Person (object)\n",
    "cardinality = \"one\"\n",
    "rule_dict = get_rules(expected_dict,[\"text\",\"person\"],cardinality)\n",
    "print(rule_dict,'\\n')\n",
    "\n",
    "## One of two potential rule types: Text (string) or URL (Formatted Text)\n",
    "cardinality = \"one\"\n",
    "rule_dict = get_rules(expected_dict,[\"text\",\"url\"],cardinality)\n",
    "print(rule_dict,'\\n')\n",
    "\n",
    "## Many of two potential rule types: Text (string) or Person (object)\n",
    "cardinality = \"Many\"\n",
    "rule_dict = get_rules(expected_dict,[\"text\",\"person\"],cardinality)\n",
    "print(rule_dict,'\\n')\n",
    "\n",
    "## Many of two potential rules types: Text (string) or URL (formatted string)\n",
    "cardinality = \"Many\"\n",
    "rule_dict = get_rules(expected_dict,[\"text\",\"url\"],cardinality)\n",
    "print(rule_dict,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate property to rule mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate mapping of properties with the same expected type\n",
    "from random import sample\n",
    "\n",
    "def clean_prop_list(stringproplist):\n",
    "    type_list = stringproplist.strip('[').strip(']').split(',')\n",
    "    clean_list = [x.strip(' ').replace(\"'\",\"\") for x in type_list]\n",
    "    rule_list = [x.lower() for x in clean_list]\n",
    "    return clean_list, rule_list\n",
    "\n",
    "def generate_id_from_proplist(stringproplist,propcount):\n",
    "    clean_list, rule_list = clean_prop_list(stringproplist)\n",
    "    idbase = [x[0] for x in clean_list]\n",
    "    letters = ['a','b','c','t','u','v','w','x','y','z']\n",
    "    randbase = sample(letters, k=5)\n",
    "    idhash = \"\".join(idbase)+'_'+str(propcount)+'_'+''.join(randbase)\n",
    "    return idhash\n",
    "    \n",
    "def generate_prop_rule_maps(datapath,expected_dict,prop_freq):\n",
    "    onemapdict = {}\n",
    "    manymapdict = {}\n",
    "    onerulemap = {}\n",
    "    manyrulemap = {}\n",
    "    onepropmap = {}\n",
    "    manypropmap = {}\n",
    "    oneproprule = {}\n",
    "    manyproprule = {}\n",
    "    prop_freq_multi = prop_freq.loc[prop_freq['counts']>1].copy() ## filter out properties that appear only once\n",
    "    for each_expected_type in prop_freq_multi['expected_types_raw'].unique().tolist():\n",
    "        propcount = prop_freq_multi.loc[prop_freq_multi['expected_types_raw']==each_expected_type]['counts'].sum()\n",
    "        proplist = prop_freq['property'].loc[prop_freq['expected_types_raw']==each_expected_type].unique().tolist()\n",
    "        clean_list, rule_list = clean_prop_list(each_expected_type)\n",
    "        idhash = generate_id_from_proplist(each_expected_type,propcount)\n",
    "        idhash_many = idhash+'_many'\n",
    "        onemapdict[idhash]=proplist\n",
    "        manymapdict[idhash_many]=proplist\n",
    "        oneruledict = get_rules(expected_dict,rule_list,\"one\")\n",
    "        onerulemap[idhash]=oneruledict\n",
    "        manyruledict = get_rules(expected_dict,rule_list,\"many\")\n",
    "        manyrulemap[idhash_many]=manyruledict\n",
    "        for eachprop in proplist:\n",
    "            onepropmap[eachprop]=idhash\n",
    "            manypropmap[eachprop]=idhash_many\n",
    "            oneproprule[eachprop] = oneruledict\n",
    "            manyproprule[eachprop] = manyruledict\n",
    "    filedict = {\"one_map.txt\":onemapdict,\"many_map.txt\":manymapdict,\"one_rule_map.txt\":onerulemap,\n",
    "                \"many_rule_map.txt\":manyrulemap,\"one_prop_map.txt\":onepropmap,\"many_prop_map.txt\":manypropmap,\n",
    "                \"one_prop_rule.txt\":oneproprule,\"many_prop_rule.txt\":manyproprule}\n",
    "    for eachkey in list(filedict.keys()):\n",
    "        with open(os.path.join(datapath,eachkey),'w+') as outfile:\n",
    "            outfile.write(json.dumps(filedict[eachkey]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join('results','mappings')\n",
    "generate_prop_rule_maps(datapath,expected_dict,prop_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check schema compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaValidationError",
     "evalue": "Class \"https://discovery.biothings.io/view/nde/Dataset\" has no path to the root \"schema:Thing\" class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSchemaValidationError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17316/2341299941.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#    url = json.load(infile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_schema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"schema.org\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\biothings_schema\\schema.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, schema, context, base_schema, validator_options)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mbase_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_schema\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_base_schema_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m# print(self.namespace, base_schema)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_schema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_schema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_schema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\biothings_schema\\schema.py\u001b[0m in \u001b[0;36mload_schema\u001b[1;34m(self, schema, base_schema, verbose)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_schema_nx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_schema_networkx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_schema_nx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema_nx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSchemaValidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_schema_nx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_schema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidator_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_full_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# split the schema networkx into individual ones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\biothings_schema\\validator.py\u001b[0m in \u001b[0;36mvalidate_full_schema\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_class_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_class_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"@id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_validation_field\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'@type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rdf:Property\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_property_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\biothings_schema\\validator.py\u001b[0m in \u001b[0;36mvalidate_validation_field\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparent_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[1;31m# raise ValueError(f'Class \"{_id}\" has no path to the root \"schema:Thing\" class')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                     self.report_validation_error(\n\u001b[0m\u001b[0;32m    289\u001b[0m                         \u001b[1;34mf'Class \"{_id}\" has no path to the root \"schema:Thing\" class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                         \u001b[0merror_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"no_path_to_root\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\biothings_schema\\validator.py\u001b[0m in \u001b[0;36mreport_validation_error\u001b[1;34m(self, err_msg, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSchemaValidationWarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'warning'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mSchemaValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_on_validation_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSchemaValidationError\u001b[0m: Class \"https://discovery.biothings.io/view/nde/Dataset\" has no path to the root \"schema:Thing\" class"
     ]
    }
   ],
   "source": [
    "from biothings_schema import Schema\n",
    "\n",
    "script_path = ''\n",
    "url = \"https://raw.githubusercontent.com/NIAID-Data-Ecosystem/nde-schemas/main/nde-mini-combined.jsonld\"\n",
    "#bioschemasfile = os.path.join(script_path,'bioschemas.json')\n",
    "#with open(bioschemasfile,'r') as infile:\n",
    "#    url = json.load(infile)\n",
    "\n",
    "sc = Schema(url, base_schema=[\"schema.org\"])\n",
    "sc.validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
